{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Setting up Pandas\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "# Setting up Seaborn\n",
    "sns.set_style(\"whitegrid\")\n",
    "#sns.set_context(\"poster\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#adding css class for better \n",
    "from IPython.core.display import HTML\n",
    "css = open('data/style-table.css').read() + open('data/style-notebook.css').read()\n",
    "HTML('<style>{}</style>'.format(css))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Available data\n",
    "\n",
    "Training set values - The independent variables for the training set.\n",
    " __'data/4910797b-ee55-40a7-8668-10efd5c1b960.csv'__\n",
    "\n",
    "Training set Labels - The dependent variable (status_group) for each of the rows in Training set values.\n",
    " __'data/0bf8bc6e-30d0-4c50-956a-603fc693d966.csv'__\n",
    "    \n",
    "Test set values - The independent variables that need predictions.\n",
    " __'data/702ddfc5-68cd-4d1d-a0de-f5f566f76d91.csv'__\n",
    "\n",
    "Submission format - The format for submitting your predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importing the Data Sets\n",
    "\n",
    "## Importing the Training set values.\n",
    "training_values = pd.read_csv('data/4910797b-ee55-40a7-8668-10efd5c1b960.csv', encoding = 'iso-8859-1')\n",
    "\n",
    "## Importing Features Data\n",
    "training_lables = pd.read_csv('data/0bf8bc6e-30d0-4c50-956a-603fc693d966.csv', encoding = 'iso-8859-1')\n",
    "\n",
    "## Importing the Testing set values for validating the trained model.\n",
    "test = pd.read_csv('data/702ddfc5-68cd-4d1d-a0de-f5f566f76d91.csv', encoding = 'iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_values.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(training_values.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Printed above are the available Features in the Training data set.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_lables.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __For the further analysis we need to combine the training_values and training_lables.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_df = pd.merge(training_values, training_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By looking at the above table we can, Most of the data available is categorial data. although lets make it sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* id can be droped because it is unique for each instance.\n",
    "* wpt_name is mostly unique values\n",
    "* num_private is ~99% zeros\n",
    "* region is highly correlated with region_code\n",
    "* quantity is highly correlated with quantity_group\n",
    "* quality_group is highly correlated with quality\n",
    "* source is highly correlated with source\n",
    "* payment is highly correlated with payment_type\n",
    "* ward need not be . included\n",
    "* extraction_type_group is highly correlated with extraction_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df = training_df.drop(['id','source','wpt_name', 'num_private', 'region', \n",
    "          'quantity'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df = training_df.drop(['quality_group','lga','ward','management', 'payment', \n",
    "           'extraction_type_group'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.population.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist1=training_df[training_df.status_group == 'functional'].construction_year\n",
    "hist2=training_df[training_df.status_group == 'functional needs repair'].construction_year\n",
    "hist3=training_df[training_df.status_group == 'non functional'].construction_year\n",
    "\n",
    "n,b,p=plt.hist([hist1, hist2, hist3], stacked=True,range=[1950,2013])\n",
    "plt.legend(['functional','functional needs repair','non functional'],loc=0)\n",
    "plt.xlabel('Construction Year', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> lets check if each column contains null values or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.apply(lambda x: sum(x.isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> cleaning the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.funder.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Keeping the top 5 values and changing others to Other\n",
    "def funder_clean(row):  \n",
    "    if row['funder']=='Government Of Tanzania':\n",
    "        return 'gov'\n",
    "    elif row['funder']=='Danida':\n",
    "        return 'danida'\n",
    "    elif row['funder']=='Hesawa':\n",
    "        return 'hesawa'\n",
    "    elif row['funder']=='Rwssp':\n",
    "        return 'rwssp'\n",
    "    elif row['funder']=='World Bank':\n",
    "        return 'world_bank'    \n",
    "    else:\n",
    "        return 'other'\n",
    "    \n",
    "training_df['funder'] = training_df.apply(lambda row: funder_clean(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Addding new column named status_vals with values to allow it to use of a pivot table to check differences\n",
    " between the different funders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replace_values = {'functional':2, 'functional needs repair':1, 'non functional':0}\n",
    "training_df['status_values']  = training_df.status_group.replace(replace_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training_df.status_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "piv_table = pd.pivot_table(training_df,index=['funder','status_group'],\n",
    "                           values='status_values', aggfunc='count')\n",
    "piv_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_danida = piv_table[0] + piv_table[1] + piv_table[2]\n",
    "percent_functional_danida = (piv_table[0] / total_danida) * 100\n",
    "\n",
    "total_gov = piv_table[3] + piv_table[4] + piv_table[5]\n",
    "percent_functional_gov = (piv_table[3] / total_gov) * 100\n",
    "\n",
    "total_hesawa = piv_table[6] + piv_table[7] + piv_table[8]\n",
    "percent_functional_hesawa = (piv_table[6] / total_hesawa) * 100\n",
    "\n",
    "total_other = piv_table[9] + piv_table[10] + piv_table[11]\n",
    "percent_functional_non_gov = (piv_table[9] / total_other) * 100\n",
    "\n",
    "total_rwssp = piv_table[12] + piv_table[13] + piv_table[14]\n",
    "percent_functional_rwssp = (piv_table[12] / total_rwssp) * 100\n",
    "\n",
    "total_world_bank = piv_table[15] + piv_table[16] + piv_table[17]\n",
    "percent_functional_world_bank = (piv_table[15] / total_world_bank) * 100\n",
    "\n",
    "print('Percent functional danida: ', round(percent_functional_danida,3))\n",
    "print('Percent functional gov: ', round(percent_functional_gov,3))\n",
    "print('Percent functional hesawa: ', round(percent_functional_hesawa,3))\n",
    "print('Percent functional non gov: ', round(percent_functional_non_gov,3))\n",
    "print('Percent functional rwssp: ', round(percent_functional_rwssp,3))\n",
    "print('Percent functional world bank: ', round(percent_functional_world_bank,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.installer.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let us do the same with all the other columns which contains null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def installer_clean(row):\n",
    "    if row['installer']=='DWE':\n",
    "        return 'dwe'\n",
    "    elif row['installer']=='Government':\n",
    "        return 'gov'\n",
    "    elif row['installer']=='RWE':\n",
    "        return 'rwe'\n",
    "    elif row['installer']=='Commu':\n",
    "        return 'commu'\n",
    "    elif row['installer']=='DANIDA':\n",
    "        return 'danida'\n",
    "    else:\n",
    "        return 'other'  \n",
    "training_df['installer'] = training_df.apply(lambda row: installer_clean(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "piv_table = pd.pivot_table(training_df,index=['installer','status_group'],\n",
    "                           values='status_values', aggfunc='count')\n",
    "piv_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_dwe = piv_table[0] + piv_table[1] + piv_table[2]\n",
    "percent_functional_dwe = (piv_table[0] / total_dwe) * 100\n",
    "\n",
    "total_gov = piv_table[3] + piv_table[4] + piv_table[5]\n",
    "percent_functional_gov = (piv_table[3] / total_gov) * 100\n",
    "\n",
    "total_hesawa = piv_table[6] + piv_table[7] + piv_table[8]\n",
    "percent_functional_hesawa = (piv_table[6] / total_hesawa) * 100\n",
    "\n",
    "total_other = piv_table[9] + piv_table[10] + piv_table[11]\n",
    "percent_functional_non_gov = (piv_table[9] / total_other) * 100\n",
    "\n",
    "total_rwssp = piv_table[12] + piv_table[13] + piv_table[14]\n",
    "percent_functional_rwssp = (piv_table[12] / total_rwssp) * 100\n",
    "\n",
    "total_world_bank = piv_table[15] + piv_table[16] + piv_table[17]\n",
    "percent_functional_world_bank = (piv_table[15] / total_world_bank) * 100\n",
    "\n",
    "print('Percent functional dwe: ', round(percent_functional_dwe,3))\n",
    "print('Percent functional gov: ', round(percent_functional_gov,3))\n",
    "print('Percent functional hesawa: ', round(percent_functional_hesawa,3))\n",
    "print('Percent functional non gov: ', round(percent_functional_non_gov,3))\n",
    "print('Percent functional rwssp: ', round(percent_functional_rwssp,3))\n",
    "print('Percent functional world bank: ', round(percent_functional_world_bank,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.subvillage.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(training_df.subvillage.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more number of unique values, we can drop it because it cannot influence more on the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_df = training_df.drop('subvillage', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.public_meeting.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##There are only two values in this column so, we can keep it and fill the null values to UNKNOWN\n",
    "training_df.public_meeting = training_df.public_meeting.fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.scheme_management.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scheme_clean(row):\n",
    "    if row['scheme_management']=='VWC':\n",
    "        return 'vwc'\n",
    "    elif row['scheme_management']=='WUG':\n",
    "        return 'wug'\n",
    "    elif row['scheme_management']=='Water authority':\n",
    "        return 'wtr_auth'\n",
    "    elif row['scheme_management']=='WUA':\n",
    "        return 'wua'\n",
    "    elif row['scheme_management']=='Water Board':\n",
    "        return 'wtr_brd'\n",
    "    else:\n",
    "        return 'other'\n",
    "training_df['scheme_management'] = training_df.apply(lambda row: scheme_clean(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "piv_table = pd.pivot_table(training_df, index=['scheme_management', 'status_group'],\n",
    "                           values='status_values', aggfunc='count')\n",
    "piv_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_other = piv_table[0] + piv_table[1] + piv_table[2]\n",
    "percent_functional_other = (piv_table[0] / total_other) * 100\n",
    "\n",
    "total_vwc = piv_table[3] + piv_table[4] + piv_table[5]\n",
    "percent_functional_vwc = (piv_table[3] / total_vwc) * 100\n",
    "\n",
    "total_wtr_auth = piv_table[6] + piv_table[7] + piv_table[8]\n",
    "percent_functional_wtr_auth = (piv_table[6] / total_wtr_auth) * 100\n",
    "\n",
    "total_wtr_brd = piv_table[9] + piv_table[10] + piv_table[11]\n",
    "percent_functional_wtr_brd = (piv_table[9] / total_wtr_brd) * 100\n",
    "\n",
    "total_wua = piv_table[12] + piv_table[13] + piv_table[14]\n",
    "percent_functional_wua = (piv_table[12] / total_wua) * 100\n",
    "\n",
    "total_wug = piv_table[15] + piv_table[16] + piv_table[17]\n",
    "percent_functional_wug = (piv_table[15] / total_wug) * 100\n",
    "\n",
    "print('Percent functional other: ', round(percent_functional_other,3))\n",
    "print('Percent functional vwc: ', round(percent_functional_vwc,3))\n",
    "print('Percent functional water authority: ', round(percent_functional_wtr_auth,3))\n",
    "print('Percent functional water board: ', round(percent_functional_wtr_brd,3))\n",
    "print('Percent functional wua: ', round(percent_functional_wua,3))\n",
    "print('Percent functional wug: ', round(percent_functional_wug,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.scheme_name.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(training_df.scheme_name.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 2697 unique values dosent make a much difference so we can drop the column.\n",
    "training_df = training_df.drop('scheme_name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.permit.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_df.permit = training_df.permit.fillna('Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> checking if there are any null values left "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.apply(lambda x: sum(x.isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> None of the columns have the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##lets check the data types again.\n",
    "cols = training_df.select_dtypes(include = ['object'])\n",
    "cols.apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Converting all the Date and time related columns to Date_time objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## changing the construction year to numeric value\n",
    "\n",
    "training_df.construction_year = pd.to_numeric(training_df.construction_year)\n",
    "training_df.construction_year.value_counts()\n",
    "## There are 20709 instances with out a year so let us fill the values with median of the column\n",
    "\n",
    "training_df['construction_year'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The median of the construction_year column is 1986, so let us fill the missing values with 1986\n",
    "training_df.loc[training_df.construction_year <= 0, training_df.columns=='construction_year'] = 1986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Converting operation time into date-time object\n",
    "\n",
    "training_df['operation_time']=training_df.date_recorded.apply(pd.to_datetime)-training_df.construction_year.apply(lambda x: pd.to_datetime(x,format='%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting the datatimeinto categorical as in :  '60s', '70s', '80s', '90s, '00s', '10s', 'unknown'.\n",
    "\n",
    "def construction_clean(row):\n",
    "    if row['construction_year'] >= 1960 and row['construction_year'] < 1970:\n",
    "        return '60s'\n",
    "    elif row['construction_year'] >= 1970 and row['construction_year'] < 1980:\n",
    "        return '70s'\n",
    "    elif row['construction_year'] >= 1980 and row['construction_year'] < 1990:\n",
    "        return '80s'\n",
    "    elif row['construction_year'] >= 1990 and row['construction_year'] < 2000:\n",
    "        return '90s'\n",
    "    elif row['construction_year'] >= 2000 and row['construction_year'] < 2010:\n",
    "        return '00s'\n",
    "    elif row['construction_year'] >= 2010:\n",
    "        return '10s'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "    \n",
    "training_df['construction_year'] = training_df.apply(lambda row: construction_clean(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Converting the Month column into date time object.\n",
    "training_df['month']=pd.to_datetime(training_df.date_recorded).dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.date_recorded = pd.to_datetime(training_df.date_recorded)\n",
    "training_df.date_recorded.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The most recent data is 2013-12-03. Subtract each date from this point to obtain a \n",
    "# 'days_since_recorded' column.\n",
    "training_df.date_recorded = pd.datetime(2013, 12, 3) - pd.to_datetime(training_df.date_recorded)\n",
    "training_df.columns = ['days_since_recorded' if x=='date_recorded' else x for x in training_df.columns]\n",
    "training_df.days_since_recorded = training_df.days_since_recorded.astype('timedelta64[D]').astype(int)\n",
    "training_df.days_since_recorded.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## basin\n",
    "training_df.basin.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "piv_table = pd.pivot_table(training_df, index=['basin', 'status_group'],\n",
    "                           values=['status_values'], aggfunc='count')\n",
    "piv_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.recorded_by.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##looks like every record is recorded by GeoData and it dosen't influence more on our model so we can drop it\n",
    "\n",
    "training_df= training_df.drop('recorded_by', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.extraction_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.extraction_type_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Keep 1 and delete the remaining\n",
    "\n",
    "training_df = training_df.drop(['extraction_type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.management_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## almost all are managed by user-group we can drop it\n",
    "training_df = training_df.drop(['management_group'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.water_quality.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.quantity_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## We can drop the location data which migh not influence on the model\n",
    "training_df = training_df.drop(['gps_height', 'longitude', 'latitude', 'region_code', 'district_code'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Dropping the status_values, operation_time, month which are added for the caluculations.\n",
    "training_df = training_df.drop(['status_values','operation_time','month'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> Now, let us check the count of each fuctional type of the pumps in status_group so that we will under stand the functional scenario of the pumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.status_group.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This gives the no.of pumps which are in each condition. now lets find out the percentaage of the pumps in each condition type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.status_group.value_counts()/len(training_df.status_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By the above result, we can say that \n",
    "there are 54.31% of Functional Pumps, 38.42% of non-functional fumpus and 7.27% of functional but which needs to be repaired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By the above figures, we can roughly estimate that there is 54.31% chance that if we take a random pump in the database to be a functional one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.countplot(training_df['status_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Matching Training and testi data frames\n",
    "test = test.drop(['gps_height', 'longitude', 'latitude', 'region_code', 'district_code',\n",
    "                  'num_private', 'id', 'payment', 'management_group', 'management', \n",
    "                  'extraction_type', 'extraction_type_group', 'recorded_by','region', 'lga',\n",
    "                  'ward', 'wpt_name', 'scheme_name', 'subvillage', 'quantity_group',\n",
    "                 'quality_group', 'source'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##cleaning the columns as the training data set\n",
    "test.date_recorded = pd.datetime(2013, 12, 3) - pd.to_datetime(test.date_recorded)\n",
    "test.columns = ['days_since_recorded' if x=='date_recorded' else x for x in test.columns]\n",
    "test.days_since_recorded = test.days_since_recorded.astype('timedelta64[D]').astype(int)\n",
    "\n",
    "test.permit = test.permit.fillna('Unknown')\n",
    "test.public_meeting = test.public_meeting.fillna('Unknown')\n",
    "\n",
    "test['scheme_management'] = test.apply(lambda row: scheme_clean(row), axis=1)\n",
    "test['construction_year'] = test.apply(lambda row: construction_clean(row), axis=1)\n",
    "test['installer'] = test.apply(lambda row: installer_clean(row), axis=1)\n",
    "test['funder'] = test.apply(lambda row: funder_clean(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_df.to_csv('training_data.csv', index=True)\n",
    "test.to_csv('test_data.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
